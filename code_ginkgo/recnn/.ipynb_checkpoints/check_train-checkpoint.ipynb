{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00353d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model\"\"\"\n",
    "\"\"\"This is from the pytorch_shuffle dir\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pickle\n",
    "import utils\n",
    "import model.data_loader as dl\n",
    "import model.dataset as dataset\n",
    "from model import recNet as net\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7fb1ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#/////////////////////    TRAINING AND EVALUATION FUNCTIONS     //////////////////////////////////////////////\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network superclass\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    ##-----------------------------\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    data_iterator_iter = iter(data_iterator)\n",
    "    \n",
    "    for i in t:\n",
    "    \n",
    "        time_before_batch=time.time() \n",
    "        \n",
    "        # fetch the next training batch\n",
    "        levels, children, n_inners, contents, n_level, labels_batch=next(data_iterator_iter)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        if params.cuda:\n",
    "          levels = levels.cuda()\n",
    "          children=children.cuda()\n",
    "          n_inners=n_inners.cuda()\n",
    "          contents=contents.cuda()\n",
    "          n_level= n_level.cuda()\n",
    "          labels_batch =labels_batch.cuda()\n",
    "      \n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        levels=torch.autograd.Variable(levels)\n",
    "        children=torch.autograd.Variable(children)\n",
    "        n_inners=torch.autograd.Variable(n_inners)\n",
    "        contents = torch.autograd.Variable(contents)\n",
    "        n_level=torch.autograd.Variable(n_level)\n",
    "        labels_batch = torch.autograd.Variable(labels_batch)    \n",
    "    \n",
    "        time_after_batch=time.time()\n",
    "#         logging.info(\"Batch creation time\" + str(time_after_batch-time_before_batch))\n",
    "        \n",
    "        ##-----------------------------\n",
    "        # Feedforward pass through the NN\n",
    "        output_batch = model(params, levels, children, n_inners, contents, n_level)\n",
    "        \n",
    "        \n",
    "        # compute model output and loss\n",
    "        labels_batch = labels_batch.float()  #Uncomment if using torch.nn.BCELoss() loss function\n",
    "        output_batch=output_batch.view((params.batch_size)) # For 1 final neuron \n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        \n",
    "        print('labels_batch=',labels_batch)\n",
    "        print('y_pred=',output_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        ##-----------------------------\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "  \n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg())) #Uncomment once tqdm is installed\n",
    "     \n",
    "#     print('summ=',summ)    \n",
    "    ##-----------------------------\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.4f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "    print('metrics_mean=',metrics_mean)\n",
    "#     print('metrics_string=',metrics_string)\n",
    "    return metrics_mean\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network superclass\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "    \n",
    "    output_all=[]\n",
    "    labels_all=[]\n",
    "    ##-----------------------------\n",
    "    # compute metrics over the dataset\n",
    "    \n",
    "    data_iterator_iter = iter(data_iterator)\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "    \n",
    "        # fetch the next evaluation batch\n",
    "        levels, children, n_inners, contents, n_level, labels_batch=next(data_iterator_iter)\n",
    "\n",
    "        # shift tensors to GPU if available\n",
    "        if params.cuda:\n",
    "          levels = levels.cuda()\n",
    "          children=children.cuda()\n",
    "          n_inners=n_inners.cuda()\n",
    "          contents=contents.cuda()\n",
    "          n_level= n_level.cuda()\n",
    "          labels_batch =labels_batch.cuda()\n",
    "\n",
    "        # convert them to Variables to record operations in the computational graph\n",
    "        levels=torch.autograd.Variable(levels)\n",
    "        children=torch.autograd.Variable(children)\n",
    "        n_inners=torch.autograd.Variable(n_inners)\n",
    "        contents = torch.autograd.Variable(contents)\n",
    "        n_level=torch.autograd.Variable(n_level)\n",
    "        labels_batch = torch.autograd.Variable(labels_batch)    \n",
    "\n",
    "        ##-----------------------------\n",
    "        # Feedforward pass through the NN\n",
    "        output_batch = model(params, levels, children, n_inners, contents, n_level)\n",
    "\n",
    "\n",
    "        # compute model output\n",
    "        labels_batch = labels_batch.float() #Uncomment if using torch.nn.BCELoss() loss function\n",
    "        output_batch=output_batch.view((params.batch_size)) # For 1 final neuron \n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        print('labels for loss=',labels_batch)\n",
    "#         print('y_pred=',output_batch)\n",
    "\n",
    "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "        # Save labels and output prob of the current batch\n",
    "        labels_all=np.concatenate((labels_all,labels_batch))        \n",
    "        output_all=np.concatenate((output_all,output_batch))\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "#         summary_batch['loss'] = loss.data[0]\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "        \n",
    "    ##-----------------------------\n",
    "    \n",
    "    ##Get the bg rejection at 30% tag eff: 0.05 + 125*(1 - 0.05)/476=0.3). That's why we pick 125\n",
    "    fpr, tpr, thresholds = roc_curve(labels_all, output_all,pos_label=1, drop_intermediate=False)\n",
    "    base_tpr = np.linspace(0.05, 1, 476)\n",
    "    inv_fpr = interp(base_tpr, tpr, 1. / fpr)[125]\n",
    "#     print('inv_fpr at 30% tag eff=',inv_fpr)\n",
    "    \n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.4f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean, inv_fpr\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, step_size, restore_file):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network superclass\n",
    "        train_data: array with levels, children, n_inners, contents, n_level and labels_batch lists\n",
    "        val_data: array levels, children, n_inners, contents, n_level and labels_batch lists\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log files\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "#     best_val_acc = np.inf\n",
    "    \n",
    "    #Save loss, accuracy history\n",
    "    history={'train_loss':[],'val_loss':[],'train_accuracy':[],'val_accuracy':[],'val_bg_reject':[]}\n",
    "    \n",
    "    ##------\n",
    "    #Create lists to access the lenght below\n",
    "    train_data=list(train_data)\n",
    "    val_data=list(val_data)    \n",
    "#    print('train data length=',len(train_data))\n",
    "#    print('train data[0]=',train_data[0])\n",
    "#    print('batch_size=',params.batch_size)\n",
    "    num_steps_train=len(train_data)//params.batch_size\n",
    "#    print('num_steps_train=',num_steps_train)\n",
    "    num_steps_val=len(val_data)//params.batch_size\n",
    "      \n",
    "    # We truncate the dataset so that we get an integer number of batches    \n",
    "    train_x=np.asarray([x for (x,y) in train_data][0:num_steps_train*params.batch_size])\n",
    "    train_y=np.asarray([y for (x,y) in train_data][0:num_steps_train*params.batch_size])        \n",
    "    val_x=np.asarray([x for (x,y) in val_data][0:num_steps_val*params.batch_size])\n",
    "    val_y=np.asarray([y for (x,y) in val_data][0:num_steps_val*params.batch_size])\n",
    "#    print('truncated train data length=',len(train_x))\n",
    "#    print('truncated label length=',len(train_y))\n",
    "    ##------\n",
    "    # Create train and val datasets. Customized dataset class: dataset.TreeDataset that will create the batches by calling data_loader.batch_nyu_pad. \n",
    "    data_train=train_x\n",
    "    labels_train=train_y\n",
    "    data_val=val_x\n",
    "    labels_val=val_y\n",
    "    transform=data_loader.batch_nyu_pad\n",
    "    batch_size=params.batch_size\n",
    "    features=params.features\n",
    "    print('transform=',transform)\n",
    "    print('features=',features)\n",
    "    shuffle_train=False\n",
    "    shuffle_val=False\n",
    "    train_data = dataset.TreeDataset(data_train,labels_train,transform,batch_size,features,shuffle_train)\n",
    "    print('train data=',train_data)\n",
    "\n",
    "    val_data = dataset.TreeDataset(data_val,labels_val,transform,batch_size,features,shuffle_val)\n",
    "  \n",
    "    ##------\n",
    "    # Create the dataloader for the train and val sets (default Pytorch dataloader). Paralelize the batch generation with num_workers. BATCH SIZE SHOULD ALWAYS BE = 1 (batches are only loaded here as a single element, and they are created with dataset.TreeDataset).\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False,\n",
    "                                               num_workers=4, pin_memory=True, collate_fn=dataset.customized_collate) \n",
    "                                               \n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False,\n",
    "                                               num_workers=4, pin_memory=True, collate_fn=dataset.customized_collate) \n",
    "    \n",
    "    ##------\n",
    "    # Train/evaluate for each epoch\n",
    "    for epoch in range(params.num_epochs):\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "        # Train one epoch\n",
    "        print('model type and dir =', type(model), dir(model))\n",
    "        print('optimizer =', optimizer)\n",
    "        print('loss_fun =', loss_fn)\n",
    "        print('train_loader type and dir =', type(train_loader),dir(train_loader))\n",
    "        print('metrics =', metrics)\n",
    "        print('params type and dir=', type(params),dir(params))\n",
    "        print('num_ateps_train =', num_steps_train)\n",
    "        train_metrics = train(model, optimizer, loss_fn, train_loader, metrics, params, num_steps_train)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        val_metrics, inv_fpr = evaluate(model, loss_fn, val_loader, metrics, params, num_steps_val)      \n",
    "\n",
    "          # Minimize the accuracy on the val set  \n",
    "#         val_acc = val_metrics['accuracy']\n",
    "#         is_best = val_acc >= best_val_acc\n",
    "\n",
    "#         \n",
    "#         # Minimize the loss on the val set\n",
    "#         val_acc = val_metrics['loss']\n",
    "#         is_best = val_acc <= best_val_acc\n",
    "        \n",
    "        \n",
    "#         # Maximize the bg rejection at 30% tag eff on the val set\n",
    "#         val_acc = inv_fpr\n",
    "#         print('val_acc=',val_acc)\n",
    "#         is_best = val_acc >= best_val_acc\n",
    "        \n",
    "#         # Save history\n",
    "#         history['train_loss'].append(train_metrics['loss'])\n",
    "#         history['val_loss'].append(val_metrics['loss'])\n",
    "#         history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "#         history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "#         history['val_bg_reject'].append(inv_fpr)\n",
    "        \n",
    "#         scheduler.step()\n",
    "#         step_size = step_size * decay\n",
    "        \n",
    "#         # Save weights\n",
    "#         utils.save_checkpoint({'epoch': epoch + 1,\n",
    "#                                'state_dict': model.state_dict(),\n",
    "#                                'optim_dict' : optimizer.state_dict()}, \n",
    "#                                is_best=is_best,\n",
    "#                                checkpoint=model_dir)\n",
    "            \n",
    "#         # If best_eval, best_save_path        \n",
    "#         if is_best:\n",
    "# #             logging.info(\"- Found new best accuracy\")\n",
    "# #             logging.info(\"- Found new lowest loss\")\n",
    "#             best_val_acc = val_acc\n",
    "#             logging.info('- Found new best bg rejection = {}'.format(best_val_acc))\n",
    "            \n",
    "#             # Save best val metrics in a json file in the model directory\n",
    "#             best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "#             utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "#         # Save latest val metrics in a json file in the model directory\n",
    "#         last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "#         utils.save_dict_to_json(val_metrics, last_json_path)\n",
    "\n",
    "#         # Save loss history in a json file in the model directory\n",
    "#         # print('loss_hist=')\n",
    "#         hist_json_path = os.path.join(model_dir, \"metrics_history.json\")\n",
    "#         utils.save_dict_list_to_json(history, hist_json_path)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6a7e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample_filename=ginkgo_kt_48jets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir= experiments/ginkgo_kt_48jets\n"
     ]
    }
   ],
   "source": [
    "algo='kt'\n",
    "sample_type = 'ginkgo'\n",
    "architecture='simpleRecNN'\n",
    "model_dir = 'experiments/ginkgo_kt_48jets'\n",
    "data_dir = '../data/preprocessed_trees/'\n",
    "restore_file = None\n",
    "\n",
    "print('Model dir=',model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "##-------------------\n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "# Load the parameters from json file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "sample_filename = model_dir.split('/')[1]\n",
    "logging.info('sample_filename={}'.format(sample_filename))\n",
    "train_data=data_dir+'train_'+sample_filename+'.pkl'\n",
    "val_data=data_dir+'dev_'+sample_filename+'.pkl'\n",
    "test_data=data_dir+'test_'+sample_filename+'.pkl'\n",
    "\n",
    "start_time = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77131f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "64c15ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Loading the datasets...\n",
      "- done loading the datasets\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##----------------------------------------------------------------------------------------------------------\n",
    "###   TRAINING\n",
    "##----------------------------------------------------------------------------------------------------------\n",
    "data_loader=dl.DataLoader # Main class with the methods to load the raw data, create and preprocess the trees\n",
    "\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "#   torch.manual_seed(230)\n",
    "#   if params.cuda: torch.cuda.manual_seed(230)\n",
    "if params.cuda: torch.cuda.seed()\n",
    "##-----------------------------\n",
    "# Create the input data pipeline \n",
    "logging.info('---'*20)\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# Load data \n",
    "with open(train_data, \"rb\") as f: train_data=pickle.load(f)\n",
    "with open(val_data, \"rb\") as f: val_data=pickle.load(f) \n",
    "\n",
    "\n",
    "logging.info(\"- done loading the datasets\") \n",
    "logging.info('---'*20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35948a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c75e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total parameters of the model= 34113\n",
      "Total weights of the model= 34113\n",
      "Model= PredictFromParticleEmbedding(\n",
      "  (fc_u): Linear(in_features=7, out_features=64, bias=True)\n",
      "  (fc_h): Linear(in_features=192, out_features=64, bias=True)\n",
      "  (transform): GRNNTransformSimple(\n",
      "    (fc_u): Linear(in_features=7, out_features=64, bias=True)\n",
      "    (fc_h): Linear(in_features=192, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "------------------------------------------------------------\n",
      "Building optimizer...\n",
      "Starting training for 40 epoch(s)\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "transform= <function DataLoader.batch_nyu_pad at 0x7f80529799d0>\n",
      "features= 7\n",
      "train data= <model.dataset.TreeDataset object at 0x7f8052a7b5e0>\n",
      "model type and dir = <class 'model.recNet.PredictFromParticleEmbedding'> ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'activation', 'add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'fc1', 'fc2', 'fc3', 'fc_h', 'fc_u', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_parameter', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'transform', 'type', 'xpu', 'zero_grad']\n",
      "optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.002\n",
      "    weight_decay: 0\n",
      ")\n",
      "loss_fun = BCELoss()\n",
      "train_loader type and dir = <class 'torch.utils.data.dataloader.DataLoader'> ['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_is_protocol', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n",
      "metrics = {'accuracy': <function accuracy at 0x7f8052979f70>}\n",
      "params type and dir= <class 'utils.Params'> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'batch_size', 'cuda', 'decay', 'dict', 'dir_name', 'features', 'hidden', 'jet_algorithm', 'learning_rate', 'myN_jets', 'name', 'nrun_finish', 'nrun_start', 'num_epochs', 'number_of_labels_types', 'sample_name', 'save', 'update']\n",
      "num_ateps_train = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/laurengreenspan/GitDLs/TreeNiNNew/code_ginkgo/recnn/model/dataset.py\", line 44, in __getitem__\n    levels, children, n_inners, contents, n_level= self.transform(self.data[index*self.batch_size:(index+1)*self.batch_size],self.features)\nTypeError: 'int' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/f6701ndd3ps4rc44csjwgnfw0000gn/T/ipykernel_32920/1894803316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lg/f6701ndd3ps4rc44csjwgnfw0000gn/T/ipykernel_32920/2698820978.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, step_size, restore_file)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params type and dir='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_ateps_train ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Evaluate for one epoch on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lg/f6701ndd3ps4rc44csjwgnfw0000gn/T/ipykernel_32920/2698820978.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# fetch the next training batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# shift tensors to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/laurengreenspan/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/laurengreenspan/GitDLs/TreeNiNNew/code_ginkgo/recnn/model/dataset.py\", line 44, in __getitem__\n    levels, children, n_inners, contents, n_level= self.transform(self.data[index*self.batch_size:(index+1)*self.batch_size],self.features)\nTypeError: 'int' object is not callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##----------------------------------------------------------------------\n",
    "## Architecture\n",
    "\n",
    "# Define the model and optimizer\n",
    "\n",
    "## a) Simple RecNN \n",
    "if architecture=='simpleRecNN': \n",
    "    print(params.cuda)\n",
    "    model = net.PredictFromParticleEmbedding(params,make_embedding=net.GRNNTransformSimple).cuda() if params.cuda else net.PredictFromParticleEmbedding(params,make_embedding=net.GRNNTransformSimple) \n",
    "\n",
    "##----\n",
    "## b) Gated RecNN\n",
    "elif architecture=='gatedRecNN':\n",
    "    model = net.PredictFromParticleEmbeddingGated(params,make_embedding=net.GRNNTransformGated).cuda() if params.cuda else net.PredictFromParticleEmbeddingGated(params,make_embedding=net.GRNNTransformGated) \n",
    "\n",
    "## c) Leaves/inner different weights -  RecNN \n",
    "elif architecture=='leaves_inner_RecNN': \n",
    "    model = net.PredictFromParticleEmbeddingLeaves(params,make_embedding=net.GRNNTransformLeaves).cuda() if params.cuda else net.PredictFromParticleEmbeddingLeaves(params,make_embedding=net.GRNNTransformLeaves) \n",
    "\n",
    "##----\n",
    "## d) Network in network (NiN) - Simple RecNN\n",
    "elif architecture=='NiNRecNN':\n",
    "    model = net.PredictFromParticleEmbeddingNiN(params,make_embedding=net.GRNNTransformSimpleNiN).cuda() if params.cuda else net.PredictFromParticleEmbeddingNiN(params,make_embedding=net.GRNNTransformSimpleNiN)  \n",
    "\n",
    "##-----\n",
    "## e) Network in network (NiN) - Simple RecNN\n",
    "elif architecture=='NiNRecNN2L3W':\n",
    "    model = net.PredictFromParticleEmbeddingNiN2L3W(params,make_embedding=net.GRNNTransformSimpleNiN2L3W).cuda() if params.cuda else net.PredictFromParticleEmbeddingNiN2L3W(params,make_embedding=net.GRNNTransformSimpleNiN2L3W)  \n",
    "\n",
    "##-----\n",
    "## f) Network in network (NiN) - Gated RecNN\n",
    "elif architecture=='NiNgatedRecNN':\n",
    "    model = net.PredictFromParticleEmbeddingGatedNiN(params,make_embedding=net.GRNNTransformGatedNiN).cuda() if params.cuda else net.PredictFromParticleEmbeddingGatedNiN(params,make_embedding=net.GRNNTransformGatedNiN) \n",
    "\n",
    "\n",
    "##-----\n",
    "## g) Network in network (NiN) -- NiN RecNN ReLU\n",
    "elif architecture=='NiNRecNNReLU':\n",
    "    model = net.PredictFromParticleEmbeddingNiNReLU(params,make_embedding=net.GRNNTransformSimpleNiNReLU).cuda() if params.cuda else net.PredictFromParticleEmbeddingNiNReLU(params,make_embedding=net.GRNNTransformSimpleNiNReLU) \n",
    "\n",
    "\n",
    "##----------------------------------------------------------------------\n",
    "# Output number of parameters of the model\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_weights = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "logging.info(\"Total parameters of the model= {}\".format(pytorch_total_params))\n",
    "logging.info(\"Total weights of the model= {}\".format(pytorch_total_weights))\n",
    "\n",
    "##----------------------------------------------------------------------\n",
    "## Optimizer and loss function\n",
    "\n",
    "logging.info(\"Model= {}\".format(model))\n",
    "logging.info(\"---\"*20)  \n",
    "logging.info(\"Building optimizer...\")\n",
    "\n",
    "step_size=params.learning_rate\n",
    "decay=params.decay\n",
    "#   optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=step_size)#,eps=1e-05)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay)\n",
    "\n",
    "# fetch loss function and metrics\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "#   loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = net.metrics\n",
    "\n",
    "##----------------------\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, step_size,restore_file)   \n",
    "\n",
    "elapsed_time=time.time()-start_time\n",
    "logging.info('Total time (minutes) ={}'.format(elapsed_time/60))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------\n",
    "###///////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "if __name__=='__main__':  \n",
    "\n",
    "  ##----------------------------------------------------------------------------------------------------------\n",
    "  # Global variables\n",
    "  ##-------------------\n",
    "\n",
    "  ##------------------------------------------------------------  \n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', default='../data/preprocessed_trees/', help=\"Directory containing the input batches\")\n",
    "\n",
    "  parser.add_argument('--model_dir', default='experiments/ginkgo_kt_48_jets/run_0', help=\"Directory containing params.json\")\n",
    "  parser.add_argument('--restore_file', default=None,\n",
    "                      help=\"Optional, name of the file in --model_dir containing weights to reload before \\\n",
    "                      training\")  # 'best' or 'last'\n",
    "\n",
    "  parser.add_argument('--jet_algorithm', help=\"jet algorithm\")\n",
    "  parser.add_argument('--architecture', default='simpleRecNN', help=\"RecNN architecture\")\n",
    "  parser.add_argument('--sample_type', default='ginkgo', help=\"sample type\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
